# Machine Learning Notebooks
Helpful jupyter noteboks that I compiled while learning Machine Learning and Deep Learning from various sources on the Internet. 

## NumPy Basics:
1. [NumPy Basics](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/00.%20NumPy%20Basics/1.%20NumPy%20Basics.ipynb)

## Data Preprocessing:
1. [Feature Selection](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/01.%20Data%20Preprocessing/1.%20Feature%20Selection.ipynb): Imputing missing values, Encoding, Binarizing.  

2. [Feature Scaling](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/01.%20Data%20Preprocessing/2.%20Scaling%2C%20Normalizing.ipynb): Min-Max Scaling, Normalizing, Standardizing. 

3. [Feature Extraction](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/01.%20Data%20Preprocessing/3.%20Feature%20Extraction.ipynb): CountVectorizer, DictVectorizer, TfidfVectorizer. 

## Regression
1. Linear & Multiple Regression

    * a. [Theory and Derivation](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/1A.%20Linear%20Regression%20and%20Gradient%20Descent%28Theory%29.ipynb)
    
    * b. [Linear Regression from scratch](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/1B.%20Linear%20Regression%20and%20Gradient%20Descent%20.ipynb)
    
    * c. [Assumptions in Linear Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/1C.%20Assumptions%20in%20Linear%20Regression%20and%20Dummy%20variables.ipynb): Assumptions in Linear Regression, Dummy Variable Trap
    
    * d. [Linear Regression using Scikit-learn](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/1C.%20Simple%20and%20Multiple%20Regression%20using%20Sci-kit%20learn.ipynb): Simple and Multivariable Regression using Scikit-learn. 

2. [Backward Elimination](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/2.%20Backward%20Elimination.ipynb): Method of Backward Elimination, P-values.

3. [Polynomial Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/3.%20Polynomial%20Regression.ipynb)

4. [Support Vector Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/4.%20Support%20Vector%20Regression.ipynb)

5. [Decision Tree Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/5.%20Decision%20Tree%20Regression.ipynb)

6. [Random Forest Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/6.%20Random%20Forest.ipynb)

7. [Robust Regression using Theil-Sen Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/8.%20Robust%20Regression%20(TheilSen%20Regressor).ipynb)

8. [Pipelines in Scikit-Learn](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/9.%20Pipelines%20in%20Sklearn.ipynb)

## Classification
1. Logistic Regression

   * a. [Logistic Regression and Gradient Descent](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/1A.%20Logistic%20Regression%20and%20Gradient%20Descent.ipynb)
   
   * b. [Deriving Logistic Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/1B.%20Deriving%20Logistic%20Regression%20.ipynb)
   
   * c. [Logistic Regression using Gradient Descent](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/1C.%20Logistic%20Regression%20using%20Gradient%20Descent.ipynb)
   
   * d. [Logistic Regression using Sci-kit learn](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/1D.%20Logistic%20Regression%20using%20Sci-kit%20learn.ipynb)

2. Regularization

   * a. [Regularization](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/2A.%20Regularization.ipynb)
   
   * b. [Regularization on Logistic Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/2B.%20Regularization%20on%20Logistic%20Regression.ipynb)
   
3. [K Nearest Neighbors](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/3.%20KNN.ipynb)

4. [Support Vector Machines](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/4.%20SVM.ipynb)

5. [Naive Bayes](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/5.%20Naive%20Bayes.ipynb)

6. [Decision Trees](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/03.%20Classification/6.%20Decision%20Trees.ipynb)

## Clustering

1. [KMeans](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/04.%20Clustering/1.%20KMeans.ipynb)

2. [Minibatch KMeans](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/04.%20Clustering/2.%20MiniBatch%20KMeans.ipynb)

3. [Hierarchical Clustering](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/04.%20Clustering/3.%20Hierarchical%20Clustering.ipynb)

4. [Application of Clustering - Image Quantization](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/04.%20Clustering/4.%20Image%20Quantization%20using%20Clustering.ipynb)

5. [Application of Custering - Outlier Detection](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/04.%20Clustering/05.%20Outlier%20Detection%20using%20KMeans.ipynb)

## Model Evalutaion

1. [Cross Validation and its types](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/05.%20Model%20Evaluation/1.%20Cross%20Validation%20and%20its%20types.ipynb)

2. [Confusion Matrix, Precision, Recall](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/05.%20Model%20Evaluation/Confusion%20Matrix%2C%20Precision%2C%20Recall.ipynb)

3. [R Squared](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/05.%20Model%20Evaluation/R%20Squared.ipynb)

4. [ROC Curve, AUC](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/05.%20Model%20Evaluation/ROC%20Curve%20%26%20AUC.ipynb)

5. [Silhoutte Distance](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/05.%20Model%20Evaluation/Silhoutte%20Distance%20for%20Clustering.ipynb)

## Associate Rule Mining

1. [Apriori Algorithm](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/06.%20Associate%20Rule%20Mining/1.%20Apriori%20Algorithm.ipynb)

2. [Eclat Model](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/06.%20Associate%20Rule%20Mining/2.%20Eclat%20Model.ipynb)

## Reinforcement Learning
1. [Upper Confidence Bound Algorithm](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/07.%20Reinforcement%20Learning/1.%20Upper%20Confidence%20Bound.ipynb)

2. [Thompson Sampling](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/07.%20Reinforcement%20Learning/2.%20Thompson%20Sampling.ipynb)

## Natural Language Processing

1. [Sentiment Analysis](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/08.%20Natural%20Language%20Processing/1.%20Sentiment%20Analysis.ipynb)

## Neural Networks

1. [What are Activation Functions](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/09.%20Neural%20Networks/1.%20Activation%20Functions.ipynb)

2. [Vanilla Neural Network](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/09.%20Neural%20Networks/2.%20ANN.ipynb)

3. [Backpropagation Derivation](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/09.%20Neural%20Networks/2A.%20Backpropagation%20.ipynb)

4. [Backpropagation in Python](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/09.%20Neural%20Networks/2B.%20Neural%20Networks%20using%20Backpropagation.ipynb)

5. [Convolutional Neural Networks](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/09.%20Neural%20Networks/3.%20Convolutional%20Neural%20Networks.ipynb)

6. [Long Short Term Memory Neural Networks (LSTM)](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/09.%20Neural%20Networks/4.%20Recurrent%20Neural%20Networks%20and%20LSTM%20(Theory).ipynb)

---

## Sources / References:
1. [Machine Learning by Andrew Ng (Coursera)](https://www.coursera.org/learn/machine-learning)
2. [Machine Learning A-Z (Udemy)](https://www.udemy.com/machinelearning/)
3. [Deep Learning A-Z (Udemy)](https://www.udemy.com/deeplearning/)
4. [Neural Networks by Geoffrey (Hinton Coursera)](https://www.coursera.org/learn/neural-networks)
5. [Scikit-learn Cookbook (Second Edition) - Julian Avila et. al](https://www.packtpub.com/big-data-and-business-intelligence/scikit-learn-cookbook-second-edition)

---
## Webminar
1. [car price prediction](https://github.com/sahidul-shaikh/car-price-prediction-linear-regression/blob/main/car-price-prediction-linear-regression.ipynb)
2. [multiple linear regression with backward propogration](https://www.kaggle.com/code/hitesh19/multi-linear-regression-with-backward-elimination)
3. [polynomial regression](https://github.com/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/3.%20Polynomial%20Regression.ipynb)
4. [linear regression toy](https://github.com/facebookarchive/tutorials/blob/master/Toy_Regression.ipynb)
5. [binary regression](https://www.kaggle.com/code/johnduva/logistic-regression-social-network-ads)
